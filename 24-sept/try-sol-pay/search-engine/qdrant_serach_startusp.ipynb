{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b847596f",
      "metadata": {
        "id": "b847596f"
      },
      "source": [
        "# Semantic Search with Vector Databases and LLM\n",
        "This example is about implementing a basic example of Semantic Search.  \n",
        "The technology is now easily available by combining frameworks and models easily available and for the most part also available as open software/resources, as well as cloud services with a subscription.  \n",
        "Semantic search can be applied to querying a set of documents. In this example we will use just one pdf document for simplicity, the article \"A Roadmap for HEP Software and Computing R&D for the 2020s\"\n",
        "\n",
        "The implementation steps are:  \n",
        "1. Take an example document and split it in chunks\n",
        "2. Create embeddings for each document chunk\n",
        "3. Store the embeddings in a Vector Database\n",
        "4. Perform semantic search using embeddings\n",
        "5. Transform the results of the search into natural language using a Large Language Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3790453",
      "metadata": {
        "id": "d3790453"
      },
      "outputs": [],
      "source": [
        "# This requires langchain and pypdf, pip install if not already available\n",
        "# !pip install langchain\n",
        "# !pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6bf3bd8",
      "metadata": {
        "scrolled": true,
        "id": "d6bf3bd8",
        "outputId": "819f5198-600c-4446-a2a2-56395058298d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  833k  100  833k    0     0   336k      0  0:00:02  0:00:02 --:--:--  336k\n"
          ]
        }
      ],
      "source": [
        "# Download the document used in this exmaple,\n",
        "# the article \"A Roadmap for HEP Software and Computing R&D for the 2020s\"\n",
        "# see https://arxiv.org/abs/1712.06982\n",
        "\n",
        "# Download a copy of the document and save it as WLCG_roadmap.pdf:\n",
        "! curl https://arxiv.org/pdf/1712.06982.pdf -o WLCG_roadmap.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b536561",
      "metadata": {
        "id": "7b536561"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"WLCG_roadmap.pdf\")\n",
        "pages = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d785f03",
      "metadata": {
        "id": "1d785f03"
      },
      "source": [
        "## 2. Create embeddings for each document chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b72d16a",
      "metadata": {
        "id": "2b72d16a"
      },
      "outputs": [],
      "source": [
        "# !pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0400697",
      "metadata": {
        "id": "f0400697"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b49b00f",
      "metadata": {
        "id": "7b49b00f"
      },
      "source": [
        "## 3. Store the embeddings in a Vector Database\n",
        "![Figure1](https://github.com/cerndb/NotebooksExamples/blob/main/AITools/Figure1_backend_preparation_vectorDB.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc972909",
      "metadata": {
        "id": "dc972909"
      },
      "source": [
        "### Option 1 (small data), use FAISS as Vector Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad1a525f",
      "metadata": {
        "id": "ad1a525f"
      },
      "outputs": [],
      "source": [
        "# This example uses FAISS and in-memory\n",
        "# !pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0463b1d",
      "metadata": {
        "scrolled": false,
        "id": "c0463b1d"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Create the embeddings and store them in an in-memory DB with FAISS\n",
        "faiss_index = FAISS.from_documents(pages, embeddings)\n",
        "\n",
        "# Optionall save the index\n",
        "faiss_index.save_local(\"faiss_index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b30f666",
      "metadata": {
        "id": "9b30f666"
      },
      "outputs": [],
      "source": [
        "# This is how you can load in the index with embeddings saved to a file\n",
        "# for future runs of the notebook\n",
        "\n",
        "# from langchain.vectorstores import FAISS\n",
        "# faiss_index = FAISS.load_local(\"faiss_index\", embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4d249a4",
      "metadata": {
        "id": "b4d249a4"
      },
      "source": [
        "### Option 2 (large data), use Open Search as Vector Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef0f011d",
      "metadata": {
        "id": "ef0f011d"
      },
      "outputs": [],
      "source": [
        "# This example uses Open Search as remote vector database\n",
        "# !pip install opensearch-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d00e998",
      "metadata": {
        "scrolled": false,
        "id": "4d00e998"
      },
      "outputs": [],
      "source": [
        "# This creates the embeddings and stored them in an Open Search index\n",
        "# For future runs of the notebook, you can skip this and link to\n",
        "# the Open Search index directly\n",
        "\n",
        "from langchain.vectorstores import OpenSearchVectorSearch\n",
        "from getpass import getpass\n",
        "\n",
        "# Contact Open Search service at CERN to get an instance and the credentials\n",
        "opensearch_url=\"https://es-testspark1.cern.ch:443/es\"\n",
        "opensearch_user=\"test1\"\n",
        "opensearch_pass = getpass()\n",
        "\n",
        "\n",
        "# perform the embeddings and store in OpenSearch\n",
        "docsearch = OpenSearchVectorSearch.from_documents(\n",
        "     documents=pages,\n",
        "     embedding=embeddings,\n",
        "     index_name=\"embd1\",\n",
        "     opensearch_url=opensearch_url,\n",
        "     http_auth=(opensearch_user, opensearch_pass),\n",
        "     use_ssl = True,\n",
        "     verify_certs = False,\n",
        "     ssl_assert_hostname = False,\n",
        "     ssl_show_warn = False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc2c52f1",
      "metadata": {
        "id": "cc2c52f1"
      },
      "outputs": [],
      "source": [
        "# This is how you can load in the index with embeddings stored to Open Search\n",
        "# for future runs of the notebook\n",
        "\n",
        "from langchain.vectorstores import OpenSearchVectorSearch\n",
        "from getpass import getpass\n",
        "\n",
        "# Open Search instance and the credentials\n",
        "opensearch_url=\"https://es-testspark1.cern.ch:443/es\"\n",
        "opensearch_user=\"test1\"\n",
        "opensearch_pass = getpass()\n",
        "\n",
        "\n",
        "# use pre-loaded embeddings in OpenSearch\n",
        "docsearch = OpenSearchVectorSearch(\n",
        "     embedding_function=embeddings,\n",
        "     index_name=\"embd1\",\n",
        "     opensearch_url=opensearch_url,\n",
        "     http_auth=(opensearch_user, opensearch_pass),\n",
        "     use_ssl = True,\n",
        "     verify_certs = False,\n",
        "     ssl_assert_hostname = False,\n",
        "     ssl_show_warn = False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "746db788",
      "metadata": {
        "id": "746db788"
      },
      "source": [
        "## 4. Perform semantic search using embeddings\n",
        "![Figure2](https://github.com/cerndb/NotebooksExamples/blob/main/AITools/Figure2_semantic_search.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "225336dd",
      "metadata": {
        "id": "225336dd"
      },
      "outputs": [],
      "source": [
        "# Choose the index you have created and want to use for this (FAISS or Open Search)\n",
        "# index = faiss_index # use FAISS in-memory index\n",
        "\n",
        "index = docsearch # use OpenSearch Index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc08486b",
      "metadata": {
        "id": "cc08486b",
        "outputId": "59acbfee-7b50-4192-c7f4-f413a1134aa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37: the same data volumes as ATLAS. The HL-LHC storage requirements per year are\n",
            "expected to jump by a factor close to 10, which is a growth rate faster than can\n",
            "be accommodated by projected technology gains. Storage will remain one of the\n",
            "major cost drivers for HEP computing, at a level roughly equal t\n",
            "37: the same data volumes as ATLAS. The HL-LHC storage requirements per year are\n",
            "expected to jump by a factor close to 10, which is a growth rate faster than can\n",
            "be accommodated by projected technology gains. Storage will remain one of the\n",
            "major cost drivers for HEP computing, at a level roughly equal t\n"
          ]
        }
      ],
      "source": [
        "# Perform a simple similarity search\n",
        "\n",
        "query = \"How will computing evolve in the next decade with LHC high luminosity?\"\n",
        "\n",
        "found_docs = index.similarity_search(query, k=2)\n",
        "\n",
        "found_docs\n",
        "for doc in found_docs:\n",
        "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300])\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43f1b538",
      "metadata": {
        "id": "43f1b538"
      },
      "source": [
        "## 5. Transform the results of the search into natural language using a Large Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c0dc431",
      "metadata": {
        "id": "6c0dc431",
        "outputId": "f24a947a-e8ca-4e72-f265-32d58d8c3dec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "········\n"
          ]
        }
      ],
      "source": [
        "# OpenAI\n",
        "#! pip install openai\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "import os\n",
        "\n",
        "from getpass import getpass\n",
        "OPENAI_API_KEY = getpass()\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "\n",
        "llm=OpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99656e8d",
      "metadata": {
        "id": "99656e8d"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2}),\n",
        "    return_source_documents=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f0ee654",
      "metadata": {
        "scrolled": false,
        "id": "8f0ee654",
        "outputId": "26957e28-1540-4b3e-bba1-834dfab51669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "query = \"How will computing evolve in the next decade with LHC high luminosity?\"\n",
        "\n",
        "result = qa({\"query\": query})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bbddec2",
      "metadata": {
        "id": "2bbddec2",
        "outputId": "fc49ce77-d14d-4605-c6d2-5fed990ec3e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' Computing will need to evolve to handle the increased data rate and volume, as well as the increased computational requirements. This will likely involve shifts in data presentation and analysis models, such as the use of event-based data streaming, and the use of new types of computing resources, such as cloud and HPC. New applications, such as training for machine learning, may also be employed to meet the computational constraints and extend physics reach.'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result['result']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0c04538",
      "metadata": {
        "id": "f0c04538"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "@webio": {
      "lastCommId": null,
      "lastKernelId": null
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}